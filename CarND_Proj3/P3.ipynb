{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use Deep Learning to Clone Driving Behavior\n",
    "\n",
    "Overview\n",
    "---\n",
    "\n",
    "In this tutorial we will train, validate and test a model using Keras to clone driving behavior.The model is composed as a supervised regression problem between the car steering angles and the road images from three different camera angles in front of the vehicle. The model will output a predicted steering angle for the autonomous vehicle within a simulated environment. Data will be collected by running a car manually in Udacity's Self-Driving Car simulator. The model performance will be tested by running the vehicle in the simulator in autonomous mode.\n",
    "\n",
    "#### The following are key objectives for this project:\n",
    "\n",
    "* Using Udacity’s Self-Driving Car simulator we will collect image data from good driving behavior\n",
    "* Implement NVIDIA’s CNN model using Keras for steering angle predictions\n",
    "* Run training and validation analysis on dataset \n",
    "* Verify model accuracy by running simulator in autonomous mode\n",
    "\n",
    "Project Deliverables\n",
    "---\n",
    "* `model.py` contains the script to create and train the keras model based on the [Nvidia architecture](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/).\n",
    "* `model.h5` is the trained keras model used to run the vehicle autonomously.\n",
    "* `drive.py` is script used to control the vehicle autonomously within the simulator.\n",
    "* `basic-track.mp4` is the recorded video produced during the autonomous run on Track 1.\n",
    "* `advanced-track.mp4` is the recorded video produced during the autonomous run on Track 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "### Install required python libraries:\n",
    "You need a [anaconda](https://www.continuum.io/downloads) or [miniconda](https://conda.io/miniconda.html) to use the environment setting.\n",
    "\n",
    "```python\n",
    "# Use TensorFlow without GPU\n",
    "conda env create -f environment.yml \n",
    "\n",
    "# Use TensorFlow with GPU\n",
    "conda env create -f environment-gpu.yml\n",
    "```\n",
    "\n",
    "Or you can manually install the required libraries (see the contents of the environemnt*.yml files) using pip.\n",
    "\n",
    "\n",
    "### Run the pretrained model\n",
    "\n",
    "Start up [the Udacity self-driving simulator](https://github.com/udacity/self-driving-car-sim), choose a scene and press the Autonomous Mode button.  Then, run the model as follows:\n",
    "\n",
    "```python\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "### To train the model\n",
    "\n",
    "You'll need the data folder which contains the training images.\n",
    "\n",
    "```python\n",
    "python model.py\n",
    "```\n",
    "\n",
    "This will generate a file `model-<epoch>.h5` whenever the performance in the epoch is better than the previous best.  For example, the first epoch will generate a file called `model-000.h5`.\n",
    "\n",
    "#### Read collected data from the simulator\n",
    " \n",
    "Image data and corresponding steering angle were collected by manually driving the vehicle in the simulator for 3 laps. The vehicle has 3 cameras to record the images - left, center, and right.  \n",
    "\n",
    " The following are the left, center, and right images recorded at the same location:\n",
    "\n",
    "![Left](https://lh3.googleusercontent.com/pqyVi_ESctbKrfwqfRtCgtN3uh-IQ1mhbHVWOzyo3Spq6I0tQwfIpDntFhkv1n-ds5tJAqoF0bfGrxDlbkD2=w1900-h853)\n",
    "![Center](https://lh5.googleusercontent.com/OIFATLQSHdSLzWpQQVfyJR6nhYIpf3l94ELAlbtQIVngjLdpqxGHJ6lcuDt0FLQBdPy4y-3Zf64hGguKayP1=w1900-h853)\n",
    "![Right](https://lh4.googleusercontent.com/_XWvU_miKZqFs7VerLAL-dabPFsONjB4JPY3xm5ovd_HpFzM3Yr_mUDlqDQpfNm5f5uloa5RoXiRL_eh2A-n=w1900-h853)\n",
    "\n",
    "####  How to run the model in autonomous mode\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n",
    "```\n",
    "python drive.py model.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy\n",
    "\n",
    "The archetecture of the model used in this turorial is based on the [Nvidia architecture](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/), which has been used by NVIDIA for the end-to-end self driving test. It is a deep CNN which works well with supervised image classification / regression problems.  It includes a normalization layer, 5 convolutional layers,1 dropout layer, and finally 3 fully connected layers. As the NVIDIA model is well documented, I was able to focus how to adjust the training images to produce the best result with some adjustments to the model to avoid overfitting and adding non-linearity to improve the prediction. Through this model, the vehicle was able to drive autonomously around the track without leaving the road or diving into water and making smooth turns around corners.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Image Sizing\n",
    "\n",
    "- the images are cropped so that the model will only consider images of the road and not the sky or other vehicle parts.\n",
    "- the images are resized to 66x200 (3 YUV channels) as per NVIDIA model\n",
    "- the images are normalized (image data divided by 127.5 and subtracted 1.0).  As stated in the Model Architecture section, this is to avoid saturation and make gradients work better)\n",
    "\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Image Augumentation\n",
    "\n",
    "For training, the following augumentation technique were used to generate sufficient amount of images:\n",
    "\n",
    "- Randomly choose right, left or center images.\n",
    "- For left image, steering angle is adjusted by +0.2\n",
    "- For right image, steering angle is adjusted by -0.2\n",
    "- Randomly flip image left/right\n",
    "- Randomly translate image horizontally with steering angle adjustment (0.002 per pixel shift)\n",
    "- Randomly translate image virtically\n",
    "- Randomly added shadows\n",
    "- Randomly altering image brightness (lighter or darker)\n",
    "\n",
    "Using the left/right images is useful to train the recovery driving scenario.  The horizontal translation is useful for difficult curve handling (i.e. the one after the bridge).\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"https://github.com/sanchezm1288/SDCND/blob/master/CarND_Proj3/basic-track.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"https://github.com/sanchezm1288/SDCND/blob/master/CarND_Proj3/basic-track.mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"https://github.com/sanchezm1288/SDCND/blob/master/CarND_Proj3/advanced-track.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"https://github.com/sanchezm1288/SDCND/blob/master/CarND_Proj3/advanced-track.mp4\">\n",
    "</video>\n",
    "\"\"\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
